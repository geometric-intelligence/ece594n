{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809d4897-8271-4a67-a49a-9ee2a597f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main package imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import functools\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from math import *\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import forge\n",
    "import forge.experiment_tools as fet\n",
    "from forge import flags\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Specific utility imports\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from oil.utils.utils import FixedNumpySeed, islice\n",
    "from oil.datasetup.datasets import IndexedDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from types import SimpleNamespace\n",
    "from einops.einops import rearrange, reduce\n",
    "from attrdict import AttrDict\n",
    "from collections import OrderedDict\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# LieConv dependencies\n",
    "from lie_conv.datasets import SpringDynamics\n",
    "from lie_conv.lieGroups import SE3\n",
    "from lie_conv.lieConv import Swish\n",
    "from lie_conv.utils import Pass, Expression\n",
    "from lie_conv.masked_batchnorm import MaskBatchNormNd\n",
    "from lie_conv.dynamicsTrainer import Partial\n",
    "from lie_conv.hamiltonian import SpringV, SpringH, HamiltonianDynamics, KeplerV, KeplerH\n",
    "from lie_conv.dynamicsTrainer import HNet\n",
    "from lie_conv.hamiltonian import HamiltonianDynamics\n",
    "from lie_conv.lieGroups import T, SE2, SE2_canonical, SO2\n",
    "\n",
    "# Import auxiliary functions \n",
    "from lie_transformer.eqv_transformer.train_tools import (\n",
    "    log_tensorboard,\n",
    "    parse_reports,\n",
    "    parse_reports_cpu,\n",
    "    print_reports,\n",
    "    load_checkpoint,\n",
    "    save_checkpoint,\n",
    "    nested_to,\n",
    "    param_count,\n",
    ")\n",
    "from lie_transformer.eqv_transformer.multihead_neural import (\n",
    "    MultiheadWeightNet,\n",
    "    MultiheadMLP,\n",
    "    LinearBNact,\n",
    "    MLP,\n",
    ")\n",
    "from lie_transformer.eqv_transformer.kernels import (\n",
    "    AttentionKernel,\n",
    "    SumKernel,\n",
    "    DotProductKernel,\n",
    "    RelativePositionKernel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535d974-71df-4a6e-a1b3-0ba69cb9c31b",
   "metadata": {},
   "source": [
    "# Model, data, training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab7c48-4797-4a1a-b0e3-f91d6f83e4fd",
   "metadata": {},
   "source": [
    "LieTransformer uses the \"forge\" library to simplify training, evaluation, and inference. The following flags allow the user to specify various settings either in the script (like we are doing here) or via the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec31b4e3-7ea2-4e92-9ed3-822325dbfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_evaluated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275a4f82-fa77-4e82-8018-2372b5cf18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration & flags\n",
    "\n",
    "if not flags_evaluated:\n",
    "    flags.DEFINE_string(\n",
    "        \"results_dir\", \"lie_transformer/checkpoints/\", \"Top directory for all experimental results.\"\n",
    "    )\n",
    "    \n",
    "    # Configuration files to load\n",
    "    flags.DEFINE_string(\n",
    "        \"data_config\",\n",
    "        \"lie_transformer/configs/dynamics/spring_dynamics_data.py\",\n",
    "        \"Path to a data config file.\",\n",
    "    )\n",
    "    flags.DEFINE_string(\n",
    "        \"model_config\",\n",
    "        \"lie_transformer/configs/dynamics/eqv_transformer_model.py\",\n",
    "        \"Path to a model config file.\",\n",
    "    )\n",
    "    # Job management\n",
    "    flags.DEFINE_string(\n",
    "        \"run_name\",\n",
    "        \"demo_run\",\n",
    "        \"Name of this job and name of results folder.\",\n",
    "    )\n",
    "    flags.DEFINE_boolean(\"resume\", False, \"Tries to resume a job if True.\")\n",
    "    \n",
    "    # Logging\n",
    "    flags.DEFINE_integer(\n",
    "        \"report_loss_every\", 10, \"Number of iterations between reporting minibatch loss.\"\n",
    "    )\n",
    "    flags.DEFINE_integer(\n",
    "        \"save_check_points\",\n",
    "        10,\n",
    "        \"frequency with which to save checkpoints, in number of epoches.\",\n",
    "    )\n",
    "    flags.DEFINE_boolean(\"log_train_values\", True, \"Logs train values if True.\")\n",
    "    flags.DEFINE_integer(\n",
    "        \"total_evaluations\",\n",
    "        100,\n",
    "        \"Maximum number of evaluations on test and validation data during training.\",\n",
    "    )\n",
    "    \n",
    "    # Optimization\n",
    "    flags.DEFINE_integer(\"train_epochs\", 3, \"Maximum number of training epochs.\")\n",
    "    flags.DEFINE_integer(\"batch_size\", 100, \"Mini-batch size.\")\n",
    "    flags.DEFINE_float(\"learning_rate\", 1e-3, \"Adam learning rate.\")\n",
    "    flags.DEFINE_float(\"beta1\", 0.9, \"Adam Beta 1 parameter\")\n",
    "    flags.DEFINE_float(\"beta2\", 0.999, \"Adam Beta 2 parameter\")\n",
    "    flags.DEFINE_string(\"lr_schedule\", \"cosine_annealing\", \"Learning rate schedule.\")\n",
    "    \n",
    "    # GPU device\n",
    "    flags.DEFINE_integer(\"device\", 0, \"GPU to use.\")\n",
    "    \n",
    "    # Debug mode tracks more stuff\n",
    "    flags.DEFINE_boolean(\"debug\", False, \"Track and show on tensorboard more metrics.\")\n",
    "    flags.DEFINE_boolean(\n",
    "        \"save_test_predictions\",\n",
    "        True,\n",
    "        \"Makes and saves test predictions on one or more test sets (e.g. 5-step and 100-step predictions) at the end of training.\",\n",
    "    )\n",
    "    flags.DEFINE_boolean(\n",
    "        \"log_val_test\", True, \"Turns off computation of validation and test errors.\"\n",
    "    )\n",
    "    \n",
    "    flags.DEFINE_string(\"group\", \"T(2)\", \"Group to be invariant to.\")\n",
    "    \n",
    "    \n",
    "    flags.DEFINE_integer(\"dim_hidden\", 160, \"Dimension of features to use in each layer\")\n",
    "    flags.DEFINE_string(\n",
    "        \"activation_function\", \"swish\", \"Activation function to use in the network\"\n",
    "    )\n",
    "    flags.DEFINE_boolean(\n",
    "        \"mean_pooling\",\n",
    "        True,\n",
    "        \"Use mean pooling insteave of sum pooling in the invariant layer\",\n",
    "    )\n",
    "    flags.DEFINE_integer(\"num_heads\", 8, \"Number of attention heads in each layer\")\n",
    "    flags.DEFINE_integer(\"kernel_dim\", 16, \"Hidden layer size to use in kernel MLPs\")\n",
    "    flags.DEFINE_integer(\"num_layers\", 5, \"Number of ResNet layers to use\")\n",
    "    flags.DEFINE_integer(\n",
    "        \"lift_samples\",\n",
    "        1,\n",
    "        \"Number of coset lift samples to use for non-trivial stabilisers.\",\n",
    "    )\n",
    "    flags.DEFINE_integer(\"model_seed\", 0, \"Model rng seed\")\n",
    "    flags.DEFINE_string(\n",
    "        \"attention_fn\", \"dot_product\", \"How to form the attention weights from the 'logits'.\"\n",
    "    )\n",
    "    \n",
    "    flags.DEFINE_string(\n",
    "        \"block_norm\", \"layer_pre\", \"Normalization to use around the attention blocks.\"\n",
    "    )\n",
    "    flags.DEFINE_string(\"output_norm\", \"none\", \"Normalization to use in final output MLP.\")\n",
    "    flags.DEFINE_string(\"kernel_norm\", \"none\", \"Normalization to use in kernel MLP.\")\n",
    "    flags.DEFINE_string(\"kernel_type\", \"mlp\", \"Attention kernel type.\")\n",
    "    flags.DEFINE_string(\"architecture\", \"model_1\", \"Overall model architecture.\")\n",
    "    flags.DEFINE_boolean(\n",
    "        \"model_with_dict\",\n",
    "        True,\n",
    "        \"Makes model output predictions in dictionary instead of directly.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    flags_evaluated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20453ec0-8b82-473d-92cc-56d79e8f519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary functions\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    reports = None\n",
    "    for data in loader:\n",
    "        data = nested_to(data, device, torch.float32)\n",
    "        outputs = model(data)\n",
    "\n",
    "        if reports is None:\n",
    "            reports = {k: v.detach().clone().cpu() for k, v in outputs.reports.items()}\n",
    "        else:\n",
    "            for k, v in outputs.reports.items():\n",
    "                reports[k] += v.detach().clone().cpu()\n",
    "\n",
    "    for k, v in reports.items():\n",
    "        reports[k] = v / len(\n",
    "            loader\n",
    "        )  \n",
    "\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea138a9-5ea5-4b43-bb2a-01b1ae55c680",
   "metadata": {},
   "source": [
    "# Construct model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634172cb-312f-4587-99f4-76bb3c1b10a4",
   "metadata": {},
   "source": [
    "## Equivariant multihead attention & equivariant transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc01c55e-63cf-41ef-82da-5fa195d0d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivairantMultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in,\n",
    "        c_out,\n",
    "        n_heads,\n",
    "        group,\n",
    "        kernel_type=\"mlp\",\n",
    "        kernel_dim=16,\n",
    "        act=\"swish\",\n",
    "        bn=False,\n",
    "        mc_samples=0,\n",
    "        fill=1.0,\n",
    "        attention_fn=\"softmax\",\n",
    "        feature_embed_dim=None,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.n_heads = n_heads\n",
    "        self.group = group\n",
    "\n",
    "        self.mc_samples = mc_samples\n",
    "        self.fill = fill\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "        if not (attention_fn in [\"softmax\", \"dot_product\", \"norm_exp\"]):\n",
    "            raise NotImplementedError(f\"{attention_fn} not implemented.\")\n",
    "        self.attention_fn = attention_fn\n",
    "\n",
    "        if len(kernel_type) == 4:\n",
    "            normalisation = [\"none\", \"softmax\", \"dot_product\"]\n",
    "            self.attention_fn = normalisation[int(kernel_type[0])]\n",
    "\n",
    "            location_feature_combination = [\"none\", \"sum\", \"mlp\", \"multiply\"]\n",
    "            location_feature_combination = location_feature_combination[\n",
    "                int(kernel_type[1])\n",
    "            ]\n",
    "\n",
    "            feature_featurisation = [\n",
    "                \"none\",\n",
    "                \"dot_product\",\n",
    "                \"linear_concat\",\n",
    "                \"linear_concat_linear\",\n",
    "            ]\n",
    "            feature_featurisation = feature_featurisation[int(kernel_type[2])]\n",
    "\n",
    "            location_featurisation = [\"none\", \"mlp\", \"none\"]\n",
    "            location_featurisation = location_featurisation[int(kernel_type[3])]\n",
    "\n",
    "            self.kernel = AttentionKernel(\n",
    "                c_in,\n",
    "                group.lie_dim + 2 * group.q_dim,\n",
    "                n_heads,\n",
    "                feature_featurisation=feature_featurisation,\n",
    "                location_featurisation=location_featurisation,\n",
    "                location_feature_combination=location_feature_combination,\n",
    "                hidden_dim=kernel_dim,\n",
    "                feature_embed_dim=feature_embed_dim,\n",
    "                activation=act,\n",
    "            )\n",
    "\n",
    "        elif kernel_type == \"mlp\":\n",
    "            self.kernel = SumKernel(\n",
    "                MultiheadWeightNet(\n",
    "                    group.lie_dim + 2 * group.q_dim,\n",
    "                    1,\n",
    "                    n_heads,\n",
    "                    hid_dim=kernel_dim,\n",
    "                    act=act,\n",
    "                    bn=bn,\n",
    "                ),\n",
    "                DotProductKernel(c_in, c_in, c_in, n_heads=n_heads),\n",
    "                n_heads,\n",
    "            )\n",
    "        elif kernel_type == \"relative_position\":\n",
    "            self.kernel = RelativePositionKernel(\n",
    "                c_in,\n",
    "                c_in,\n",
    "                group.lie_dim + 2 * group.q_dim,\n",
    "                n_heads=n_heads,\n",
    "                bias=True,\n",
    "                lamda=1.0,\n",
    "            )\n",
    "        elif kernel_type == \"dot_product_only\":\n",
    "            self.kernel = SumKernel(\n",
    "                lambda x: [\n",
    "                    None,\n",
    "                    torch.zeros(x[1].shape[:-1], device=x[2].device).unsqueeze(-1),\n",
    "                    None,\n",
    "                ],  # unsure what's going on here. Dims don't match so trying to fix it.\n",
    "                DotProductKernel(c_in, c_in, c_in, n_heads=n_heads),\n",
    "                n_heads,\n",
    "            )\n",
    "        elif kernel_type == \"location_only\":\n",
    "            self.kernel = SumKernel(\n",
    "                MultiheadWeightNet(\n",
    "                    group.lie_dim + 2 * group.q_dim,\n",
    "                    1,\n",
    "                    n_heads,\n",
    "                    hid_dim=kernel_dim,\n",
    "                    act=act,\n",
    "                    bn=bn,\n",
    "                ),\n",
    "                lambda x1, x2, x3: 0,\n",
    "                n_heads,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"{kernel_type} is not a valid kernel type\")\n",
    "\n",
    "        self.input_linear = nn.Linear(c_in, c_out)\n",
    "        self.output_linear = nn.Linear(c_out, c_out)\n",
    "\n",
    "    def extract_neighbourhoods(self, input, query_indices=None):\n",
    "        \"\"\"Extracts which points each other point is to attend to based on distance, or graph structure\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : (pairwise_g, coset_functions, mask)\n",
    "        \"\"\"\n",
    "        # TODO: Currently no down sampling in this step.\n",
    "\n",
    "        pairwise_g, coset_functions, mask = input\n",
    "\n",
    "        if query_indices is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            coset_functions_at_query = coset_functions\n",
    "            mask_at_query = mask\n",
    "            pairwise_g_at_query = pairwise_g\n",
    "\n",
    "        if self.mc_samples > 0:\n",
    "            dists = self.group.distance(pairwise_g_at_query)\n",
    "            dists = torch.where(\n",
    "                mask[:, None, :].expand(*dists.shape),\n",
    "                dists,\n",
    "                1e8 * torch.ones_like(dists),\n",
    "            )\n",
    "            k = (\n",
    "                coset_functions.shape[1]\n",
    "                if not self.mc_samples\n",
    "                else min(self.mc_samples, coset_functions.shape[1])\n",
    "            )\n",
    "            k_ball = (\n",
    "                coset_functions.shape[1]\n",
    "                if not self.mc_samples\n",
    "                else min(int(self.mc_samples / self.fill), coset_functions.shape[1])\n",
    "            )\n",
    "            _, points_in_ball_indices = dists.topk(\n",
    "                k=k_ball, dim=-1, largest=False, sorted=False\n",
    "            )\n",
    "            ball_indices = torch.randperm(k_ball)[:k]\n",
    "\n",
    "            nbhd_idx = points_in_ball_indices[:, :, ball_indices]\n",
    "\n",
    "        else:\n",
    "            nbhd_idx = (\n",
    "                torch.arange(coset_functions.shape[1], device=coset_functions.device)\n",
    "                .long()[None, None, :]\n",
    "                .expand(pairwise_g.shape[:-1])\n",
    "            )\n",
    "\n",
    "        # Get batch index array\n",
    "        BS = (\n",
    "            torch.arange(coset_functions.shape[0], device=coset_functions.device)\n",
    "            .long()[:, None, None]\n",
    "            .expand(*nbhd_idx.shape)\n",
    "        )\n",
    "        # Get NNS indexes\n",
    "        NNS = (\n",
    "            torch.arange(coset_functions.shape[1], device=coset_functions.device)\n",
    "            .long()[None, :, None]\n",
    "            .expand(*nbhd_idx.shape)\n",
    "        )\n",
    "\n",
    "        nbhd_pairwise_g = pairwise_g[\n",
    "            BS, NNS, nbhd_idx\n",
    "        ]  # (bs, n * ns, n * ns, g_dim) -> (bs, n * ns, nbhd_size, g_dim)\n",
    "        # nbhd_coset_functions = coset_functions[\n",
    "        #     BS, nbhd_idx\n",
    "        # ]  # (bs, n * ns, c_in) -> (bs, n * ns, nbhd_size, c_in)\n",
    "        nbhd_mask = mask[BS, nbhd_idx]  # (bs, n * ns) -> (bs, n * ns, nbhd_size)\n",
    "\n",
    "        # (bs, n * ns, nbhd_size, g_dim), (bs, n * ns, nbhd_size, c_in), (bs, n * ns, nbhd_size), (bs, n * ns, nbhd_size)\n",
    "        return (\n",
    "            nbhd_pairwise_g,\n",
    "            None,\n",
    "            nbhd_mask,\n",
    "            nbhd_idx,\n",
    "            BS,\n",
    "            NNS,\n",
    "        )  # TODO: last two are conveniences - is there an easier way to do this?\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # (bs, n * ns, n * ns, g_dim), (bs, n * ns, c_in), (bs, n * ns)\n",
    "        pairwise_g, coset_functions, mask = input\n",
    "        bs, n, d = coset_functions.shape\n",
    "\n",
    "        # (bs, n * ns, nbhd_size, g_dim), (bs, n * ns, nbhd_size, c_in), (bs, n * ns, nbhd_size), (bs, n * ns, nbhd_size)\n",
    "        (\n",
    "            nbhd_pairwise_g,\n",
    "            nbhd_coset_functions,\n",
    "            nbhd_mask,\n",
    "            nbhd_idx,\n",
    "            BS,\n",
    "            NNS,\n",
    "        ) = self.extract_neighbourhoods(input)\n",
    "\n",
    "        # (bs, n * ns, n * ns, g_dim), (bs, n * ns, c_in), (bs, n * ns, nbhd_size, c_in) -> (bs, n * ns, nbhd_size, h)\n",
    "        presoftmax_weights = self.kernel(\n",
    "            nbhd_pairwise_g, nbhd_mask, coset_functions, coset_functions, nbhd_idx\n",
    "        )\n",
    "\n",
    "        if self.attention_fn == \"softmax\":\n",
    "            # Make masked areas very small attention weights\n",
    "            presoftmax_weights = torch.where(\n",
    "                # (bs, n * ns, nbhd_size) -> (bs, n * ns, nbhd_size, 1). Constant along head dim\n",
    "                nbhd_mask.unsqueeze(-1),\n",
    "                presoftmax_weights,\n",
    "                torch.tensor(\n",
    "                    -1e38,\n",
    "                    dtype=presoftmax_weights.dtype,\n",
    "                    device=presoftmax_weights.device,\n",
    "                )\n",
    "                * torch.ones_like(presoftmax_weights),\n",
    "            )\n",
    "\n",
    "            # Compute the normalised attention weights\n",
    "            # (bs, n * ns, nbhd_size, h) -> (bs, n * ns, nbhd_size, h)\n",
    "            attention_weights = F.softmax(presoftmax_weights, dim=2)\n",
    "\n",
    "        elif self.attention_fn == \"norm_exp\":\n",
    "            # Make masked areas very small attention weights\n",
    "            presoftmax_weights = torch.where(\n",
    "                # (bs, n * ns, nbhd_size) -> (bs, n * ns, nbhd_size, 1). Constant along head dim\n",
    "                nbhd_mask.unsqueeze(-1),\n",
    "                presoftmax_weights,\n",
    "                torch.tensor(\n",
    "                    -1e38,\n",
    "                    dtype=presoftmax_weights.dtype,\n",
    "                    device=presoftmax_weights.device,\n",
    "                )\n",
    "                * torch.ones_like(presoftmax_weights),\n",
    "            )\n",
    "\n",
    "            # Compute the normalised attention weights\n",
    "            # (bs, n * ns, nbhd_size, h) -> (bs, n * ns, nbhd_size, h)\n",
    "            attention_weights = presoftmax_weights.exp()\n",
    "            normalization = nbhd_mask.unsqueeze(-1).sum(-2, keepdim=True)\n",
    "            normalization = torch.clamp(normalization, min=1)\n",
    "            attention_weights = attention_weights / normalization\n",
    "\n",
    "        # From the non-local attention paper\n",
    "        elif self.attention_fn == \"dot_product\":\n",
    "            attention_weights = torch.where(\n",
    "                # (bs, n * ns, nbhd_size) -> (bs, n * ns, nbhd_size, 1). Constant along head dim\n",
    "                nbhd_mask.unsqueeze(-1),\n",
    "                presoftmax_weights,\n",
    "                torch.tensor(\n",
    "                    0.0,\n",
    "                    dtype=presoftmax_weights.dtype,\n",
    "                    device=presoftmax_weights.device,\n",
    "                )\n",
    "                * torch.ones_like(presoftmax_weights),\n",
    "            )\n",
    "\n",
    "            normalization = nbhd_mask.unsqueeze(-1).sum(-2, keepdim=True)\n",
    "            normalization = torch.clamp(normalization, min=1)\n",
    "\n",
    "            # Compute the normalised attention weights\n",
    "            # (bs, n * ns, nbhd_size, h) -> (bs, n * ns, nbhd_size, h)\n",
    "            attention_weights = attention_weights / normalization\n",
    "\n",
    "        # Pass the inputs through the value linear layer\n",
    "        # (bs, n * ns, nbhd_size, c_in) -> (bs, n * ns, nbhd_size, c_out)\n",
    "        coset_functions = self.input_linear(coset_functions)\n",
    "\n",
    "        attention_weights_expanded = torch.zeros(\n",
    "            (bs, n, n, self.n_heads),\n",
    "            dtype=attention_weights.dtype,\n",
    "            device=attention_weights.device,\n",
    "        )\n",
    "\n",
    "        # (bs, n, n, h) hopefully?\n",
    "        attention_weights_expanded[BS, NNS, nbhd_idx] = attention_weights\n",
    "        attention_weights_expanded = rearrange(\n",
    "            attention_weights_expanded, \"b n m h -> b h n m\"\n",
    "        )\n",
    "\n",
    "        coset_functions = rearrange(\n",
    "            coset_functions, \"b m (h d) -> b h m d\", h=self.n_heads\n",
    "        )\n",
    "\n",
    "        coset_functions = attention_weights_expanded.matmul(coset_functions)\n",
    "        coset_functions = rearrange(coset_functions, \"b h n d -> b n (h d)\")\n",
    "\n",
    "        coset_functions = self.output_linear(coset_functions)\n",
    "\n",
    "        # ( (bs, n * ns, n * ns, g_dim), (bs, n * ns, c_out), (bs, n * ns) )\n",
    "        return (pairwise_g, coset_functions, mask)\n",
    "\n",
    "\n",
    "class EquivariantTransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_heads,\n",
    "        group,\n",
    "        block_norm=\"layer_pre\",\n",
    "        kernel_norm=\"none\",\n",
    "        kernel_type=\"mlp\",\n",
    "        kernel_dim=16,\n",
    "        kernel_act=\"swish\",\n",
    "        hidden_dim_factor=1,\n",
    "        mc_samples=0,\n",
    "        fill=1.0,\n",
    "        attention_fn=\"softmax\",\n",
    "        feature_embed_dim=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ema = EquivairantMultiheadAttention(\n",
    "            dim,\n",
    "            dim,\n",
    "            n_heads,\n",
    "            group,\n",
    "            kernel_type=kernel_type,\n",
    "            kernel_dim=kernel_dim,\n",
    "            act=kernel_act,\n",
    "            bn=kernel_norm == \"batch\",\n",
    "            mc_samples=mc_samples,\n",
    "            fill=fill,\n",
    "            attention_fn=attention_fn,\n",
    "            feature_embed_dim=feature_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(dim, dim, dim, 2, kernel_act, kernel_norm == \"batch\")\n",
    "\n",
    "        if block_norm == \"none\":\n",
    "            self.attention_function = lambda inpt: inpt[1] + self.ema(inpt)[1]\n",
    "            self.mlp_function = lambda inpt: inpt[1] + self.mlp(inpt)[1]\n",
    "        elif block_norm == \"layer_pre\":\n",
    "            self.ln_ema = nn.LayerNorm(dim)\n",
    "            self.ln_mlp = nn.LayerNorm(dim)\n",
    "\n",
    "            self.attention_function = (\n",
    "                lambda inpt: inpt[1]\n",
    "                + self.ema((inpt[0], self.ln_ema(inpt[1]), inpt[2]))[1]\n",
    "            )\n",
    "            self.mlp_function = (\n",
    "                lambda inpt: inpt[1]\n",
    "                + self.mlp((inpt[0], self.ln_mlp(inpt[1]), inpt[2]))[1]\n",
    "            )\n",
    "        elif block_norm == \"layer_post\":\n",
    "            self.ln_ema = nn.LayerNorm(dim)\n",
    "            self.ln_mlp = nn.LayerNorm(dim)\n",
    "\n",
    "            self.attention_function = lambda inpt: inpt[1] + self.ln_ema(\n",
    "                self.ema(inpt)[1]\n",
    "            )\n",
    "            self.mlp_function = lambda inpt: inpt[1] + self.ln_mlp(self.mlp(inpt)[1])\n",
    "        elif block_norm == \"batch_pre\":\n",
    "            self.bn_ema = MaskBatchNormNd(dim)\n",
    "            self.bn_mlp = MaskBatchNormNd(dim)\n",
    "\n",
    "            self.attention_function = (\n",
    "                lambda inpt: inpt[1] + self.ema(self.bn_ema(inpt))[1]\n",
    "            )\n",
    "            self.mlp_function = lambda inpt: inpt[1] + self.mlp(self.bn_mlp(inpt))[1]\n",
    "        elif block_norm == \"batch_post\":\n",
    "            self.bn_ema = MaskBatchNormNd(dim)\n",
    "            self.bn_mlp = MaskBatchNormNd(dim)\n",
    "\n",
    "            self.attention_function = (\n",
    "                lambda inpt: inpt[1] + self.bn_ema(self.ema(inpt))[1]\n",
    "            )\n",
    "            self.mlp_function = lambda inpt: inpt[1] + self.bn_mlp(self.mlp(inpt))[1]\n",
    "        else:\n",
    "            raise ValueError(f\"{block_norm} is invalid block norm type.\")\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        inpt[1] = self.attention_function(inpt)\n",
    "        inpt[1] = self.mlp_function(inpt)\n",
    "\n",
    "        return inpt\n",
    "\n",
    "\n",
    "class GlobalPool(nn.Module):\n",
    "    \"\"\"computes values reduced over all spatial locations (& group elements) in the mask\"\"\"\n",
    "\n",
    "    def __init__(self, mean=False):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x [xyz (bs,n,d), vals (bs,n,c), mask (bs,n)]\"\"\"\n",
    "        if len(x) == 2:\n",
    "            return x[1].mean(1)\n",
    "        coords, vals, mask = x\n",
    "\n",
    "        if self.mean:\n",
    "            # mean pooling\n",
    "            summed = torch.where(mask.unsqueeze(-1), vals, torch.zeros_like(vals)).sum(\n",
    "                1\n",
    "            )\n",
    "            summed_mask = mask.sum(-1).unsqueeze(-1)\n",
    "            summed_mask = torch.where(\n",
    "                summed_mask == 0, torch.ones_like(summed_mask), summed_mask\n",
    "            )\n",
    "            summed /= summed_mask\n",
    "\n",
    "            return summed\n",
    "        else:\n",
    "            # max pooling\n",
    "            masked = torch.where(\n",
    "                mask.unsqueeze(-1),\n",
    "                vals,\n",
    "                torch.tensor(\n",
    "                    -1e38,\n",
    "                    dtype=vals.dtype,\n",
    "                    device=vals.device,\n",
    "                )\n",
    "                * torch.ones_like(vals),\n",
    "            )\n",
    "\n",
    "            return masked.max(dim=1)[0]\n",
    "\n",
    "\n",
    "class EquivariantTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input,\n",
    "        dim_output,\n",
    "        dim_hidden,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        global_pool=True,\n",
    "        global_pool_mean=True,\n",
    "        group=SE3(0.2),\n",
    "        liftsamples=1,\n",
    "        block_norm=\"layer_pre\",\n",
    "        output_norm=\"none\",\n",
    "        kernel_norm=\"none\",\n",
    "        kernel_type=\"mlp\",\n",
    "        kernel_dim=16,\n",
    "        kernel_act=\"swish\",\n",
    "        mc_samples=0,\n",
    "        fill=1.0,\n",
    "        architecture=\"model_1\",\n",
    "        attention_fn=\"softmax\",  # softmax or dot product? SZ: TODO: \"dot product\" is used to describe both the attention weights being non-softmax (non-local attention paper) and the feature kernel. should fix terminology\n",
    "        feature_embed_dim=None,\n",
    "        max_sample_norm=None,\n",
    "        lie_algebra_nonlinearity=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(dim_hidden, int):\n",
    "            dim_hidden = [dim_hidden] * (num_layers + 1)\n",
    "\n",
    "        if isinstance(num_heads, int):\n",
    "            num_heads = [num_heads] * num_layers\n",
    "\n",
    "        attention_block = lambda dim, n_head: EquivariantTransformerBlock(\n",
    "            dim,\n",
    "            n_head,\n",
    "            group,\n",
    "            block_norm=block_norm,\n",
    "            kernel_norm=kernel_norm,\n",
    "            kernel_type=kernel_type,\n",
    "            kernel_dim=kernel_dim,\n",
    "            kernel_act=kernel_act,\n",
    "            mc_samples=mc_samples,\n",
    "            fill=fill,\n",
    "            attention_fn=attention_fn,\n",
    "            feature_embed_dim=feature_embed_dim,\n",
    "        )\n",
    "\n",
    "        activation_fn = {\n",
    "            \"swish\": Swish,\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"softplus\": nn.Softplus,\n",
    "        }\n",
    "\n",
    "        if architecture == \"model_1\":\n",
    "            if output_norm == \"batch\":\n",
    "                norm1 = nn.BatchNorm1d(dim_hidden[-1])\n",
    "                norm2 = nn.BatchNorm1d(dim_hidden[-1])\n",
    "                norm3 = nn.BatchNorm1d(dim_hidden[-1])\n",
    "            elif output_norm == \"layer\":\n",
    "                norm1 = nn.LayerNorm(dim_hidden[-1])\n",
    "                norm2 = nn.LayerNorm(dim_hidden[-1])\n",
    "                norm3 = nn.LayerNorm(dim_hidden[-1])\n",
    "            elif output_norm == \"none\":\n",
    "                norm1 = nn.Sequential()\n",
    "                norm2 = nn.Sequential()\n",
    "                norm3 = nn.Sequential()\n",
    "            else:\n",
    "                raise ValueError(f\"{output_norm} is not a valid norm type.\")\n",
    "\n",
    "            self.net = nn.Sequential(\n",
    "                Pass(nn.Linear(dim_input, dim_hidden[0]), dim=1),\n",
    "                *[\n",
    "                    attention_block(dim_hidden[i], num_heads[i])\n",
    "                    for i in range(num_layers)\n",
    "                ],\n",
    "                GlobalPool(mean=global_pool_mean)\n",
    "                if global_pool\n",
    "                else Expression(lambda x: x[1]),\n",
    "                nn.Sequential(\n",
    "                    norm1,\n",
    "                    activation_fn[kernel_act](),\n",
    "                    nn.Linear(dim_hidden[-1], dim_hidden[-1]),\n",
    "                    norm2,\n",
    "                    activation_fn[kernel_act](),\n",
    "                    nn.Linear(dim_hidden[-1], dim_hidden[-1]),\n",
    "                    norm3,\n",
    "                    activation_fn[kernel_act](),\n",
    "                    nn.Linear(dim_hidden[-1], dim_output),\n",
    "                ),\n",
    "            )\n",
    "        elif architecture == \"lieconv\":\n",
    "            if output_norm == \"batch\":\n",
    "                norm = nn.BatchNorm1d(dim_hidden[-1])\n",
    "            elif output_norm == \"none\":\n",
    "                norm = nn.Sequential()\n",
    "            else:\n",
    "                raise ValueError(f\"{output_norm} is not a valid norm type.\")\n",
    "\n",
    "            self.net = nn.Sequential(\n",
    "                Pass(nn.Linear(dim_input, dim_hidden[0]), dim=1),\n",
    "                *[\n",
    "                    attention_block(dim_hidden[i], num_heads[i])\n",
    "                    for i in range(num_layers)\n",
    "                ],\n",
    "                nn.Sequential(\n",
    "                    OrderedDict(\n",
    "                        [\n",
    "                            # (\"norm\", Pass(norm, dim=1)),\n",
    "                            (\n",
    "                                \"activation\",\n",
    "                                Pass(\n",
    "                                    activation_fn[kernel_act](),\n",
    "                                    dim=1,\n",
    "                                ),\n",
    "                            ),\n",
    "                            (\n",
    "                                \"linear\",\n",
    "                                Pass(nn.Linear(dim_hidden[-1], dim_output), dim=1),\n",
    "                            ),\n",
    "                        ]\n",
    "                    )\n",
    "                ),\n",
    "                GlobalPool(mean=global_pool_mean)\n",
    "                if global_pool\n",
    "                else Expression(lambda x: x[1]),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"{architecture} is not a valid architecture.\")\n",
    "\n",
    "        self.group = group\n",
    "        self.liftsamples = liftsamples\n",
    "        self.max_sample_norm = max_sample_norm\n",
    "\n",
    "        self.lie_algebra_nonlinearity = lie_algebra_nonlinearity\n",
    "        if lie_algebra_nonlinearity is not None:\n",
    "            if lie_algebra_nonlinearity == \"tanh\":\n",
    "                self.lie_algebra_nonlinearity = nn.Tanh()\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{lie_algebra_nonlinearity} is not a supported nonlinearity\"\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.max_sample_norm is None:\n",
    "            lifted_data = self.group.lift(input, self.liftsamples)\n",
    "        else:\n",
    "            lifted_data = [\n",
    "                torch.tensor(self.max_sample_norm * 2, device=input[0].device),\n",
    "                0,\n",
    "                0,\n",
    "            ]\n",
    "            while lifted_data[0].norm(dim=-1).max() > self.max_sample_norm:\n",
    "                lifted_data = self.group.lift(input, self.liftsamples)\n",
    "\n",
    "        if self.lie_algebra_nonlinearity is not None:\n",
    "            lifted_data = list(lifted_data)\n",
    "            pairs_norm = lifted_data[0].norm(dim=-1) + 1e-6\n",
    "            lifted_data[0] = lifted_data[0] * (\n",
    "                self.lie_algebra_nonlinearity(pairs_norm / 7) / pairs_norm\n",
    "            ).unsqueeze(-1)\n",
    "\n",
    "        return self.net(lifted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ccddf-8520-4bfc-ab0f-9e312189747f",
   "metadata": {},
   "source": [
    "## Construct dynamics predictor (specific application of EquivariantTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518a15e4-1054-4cba-b85d-047402b8f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsPredictor(nn.Module):\n",
    "    \"\"\"This class implements forward pass through our model, including loss computation.\"\"\"\n",
    "\n",
    "    def __init__(self, predictor, debug=False, task=\"spring\", model_with_dict=True):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor\n",
    "        self.debug = debug\n",
    "        self.task = task\n",
    "        self.model_with_dict = model_with_dict\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"DynamicsPredictor is in DEBUG MODE.\")\n",
    "\n",
    "    def _rollout_model(self, z0, ts, sys_params, tol=1e-4):\n",
    "        \"\"\"inputs [z0: (bs, z_dim), ts: (bs, T), sys_params: (bs, n, c)]\n",
    "        outputs pred_zs: (bs, T, z_dim)\"\"\"\n",
    "        dynamics = Partial(self.predictor, sysP=sys_params)\n",
    "        zs = odeint(dynamics, z0, ts[0], rtol=tol, method=\"rk4\")\n",
    "        return zs.permute(1, 0, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        o = AttrDict()\n",
    "\n",
    "        (z0, sys_params, ts), true_zs = data\n",
    "\n",
    "        pred_zs = self._rollout_model(z0, ts, sys_params)\n",
    "        mse = (pred_zs - true_zs).pow(2).mean()\n",
    "\n",
    "        if self.debug:\n",
    "            if self.task == \"spring\":\n",
    "                # currently a bit inefficient to do the below?\n",
    "                with torch.no_grad():\n",
    "                    (z0, sys_params, ts), true_zs = data\n",
    "\n",
    "                    z = z0\n",
    "                    m = sys_params[..., 0]  # assume the first component encodes masses\n",
    "                    D = z.shape[-1]  # of ODE dims, 2*num_particles*space_dim\n",
    "                    q = z[:, : D // 2].reshape(*m.shape, -1)\n",
    "                    p = z[:, D // 2 :].reshape(*m.shape, -1)\n",
    "                    V_pred = self.predictor.compute_V((q, sys_params))\n",
    "\n",
    "                    k = sys_params[..., 1]\n",
    "                    V_true = SpringV(q, k)\n",
    "\n",
    "                    mse_V = (V_pred - V_true).pow(2).mean()\n",
    "\n",
    "                    # dynamics\n",
    "                    dyn_tz_pred = self.predictor(ts, z0, sys_params)\n",
    "\n",
    "                    H = lambda t, z: SpringH(\n",
    "                        z, sys_params[..., 0].squeeze(-1), sys_params[..., 1].squeeze(-1)\n",
    "                    )\n",
    "                    dynamics = HamiltonianDynamics(H, wgrad=False)\n",
    "                    dyn_tz_true = dynamics(ts, z0)\n",
    "\n",
    "                    mse_dyn = (dyn_tz_true - dyn_tz_pred).pow(2).mean()\n",
    "\n",
    "            o.mse_dyn = mse_dyn\n",
    "            o.mse_V = mse_V\n",
    "\n",
    "        o.prediction = pred_zs\n",
    "        o.mse = mse\n",
    "        o.loss = mse  # loss wrt which we train the model\n",
    "\n",
    "        if self.debug:\n",
    "            o.reports = AttrDict({\"mse\": o.mse, \"mse_V\": o.mse_V, \"mse_dyn\": o.mse_dyn})\n",
    "        else:\n",
    "            o.reports = AttrDict({\"mse\": o.mse})\n",
    "\n",
    "        if not self.model_with_dict:\n",
    "            return pred_zs\n",
    "\n",
    "        return o\n",
    "\n",
    "class DynamicsEquivariantTransformer(EquivariantTransformer, HNet):\n",
    "    def __init__(self, center=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.center = center\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, z, sysP, wgrad=True):\n",
    "        dynamics = HamiltonianDynamics(\n",
    "            lambda t, z: self.compute_H(z, sysP), wgrad=wgrad\n",
    "        )\n",
    "        return dynamics(t, z)\n",
    "\n",
    "    def compute_V(self, x):\n",
    "        \"\"\"Input is a canonical position variable and the system parameters,\n",
    "        shapes (bs, n,d) and (bs,n,c)\"\"\"\n",
    "        q, sys_params = x\n",
    "        mask = ~torch.isnan(q[..., 0])\n",
    "        if self.center:\n",
    "            q = q - q.mean(1, keepdims=True)\n",
    "        return super().forward((q, sys_params, mask)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39fb15-0014-4a86-ac59-81e0c5612481",
   "metadata": {},
   "source": [
    "## Construction of specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f9b39b-962f-4bca-8bd5-0c392c167e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicsPredictor(\n",
       "  (predictor): DynamicsEquivariantTransformer(\n",
       "    (net): Sequential(\n",
       "      (0): Pass(\n",
       "        (module): Linear(in_features=2, out_features=160, bias=True)\n",
       "      )\n",
       "      (1): EquivariantTransformerBlock(\n",
       "        (ema): EquivairantMultiheadAttention(\n",
       "          (kernel): SumKernel(\n",
       "            (location_kernel): Sequential(\n",
       "              (LinNormAct_1): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_2): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_3): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (feature_kernel): DotProductKernel(\n",
       "              (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
       "              (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (input_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (output_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (LinNormAct_1): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "          (LinNormAct_2): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_ema): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_mlp): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EquivariantTransformerBlock(\n",
       "        (ema): EquivairantMultiheadAttention(\n",
       "          (kernel): SumKernel(\n",
       "            (location_kernel): Sequential(\n",
       "              (LinNormAct_1): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_2): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_3): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (feature_kernel): DotProductKernel(\n",
       "              (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
       "              (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (input_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (output_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (LinNormAct_1): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "          (LinNormAct_2): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_ema): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_mlp): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EquivariantTransformerBlock(\n",
       "        (ema): EquivairantMultiheadAttention(\n",
       "          (kernel): SumKernel(\n",
       "            (location_kernel): Sequential(\n",
       "              (LinNormAct_1): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_2): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_3): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (feature_kernel): DotProductKernel(\n",
       "              (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
       "              (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (input_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (output_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (LinNormAct_1): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "          (LinNormAct_2): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_ema): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_mlp): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): EquivariantTransformerBlock(\n",
       "        (ema): EquivairantMultiheadAttention(\n",
       "          (kernel): SumKernel(\n",
       "            (location_kernel): Sequential(\n",
       "              (LinNormAct_1): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_2): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_3): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (feature_kernel): DotProductKernel(\n",
       "              (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
       "              (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (input_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (output_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (LinNormAct_1): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "          (LinNormAct_2): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_ema): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_mlp): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): EquivariantTransformerBlock(\n",
       "        (ema): EquivairantMultiheadAttention(\n",
       "          (kernel): SumKernel(\n",
       "            (location_kernel): Sequential(\n",
       "              (LinNormAct_1): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_2): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "              (LinNormAct_3): Sequential(\n",
       "                (linear): Pass(\n",
       "                  (module): MultiheadLinear()\n",
       "                )\n",
       "                (norm): Sequential()\n",
       "                (activation): Pass(\n",
       "                  (module): Expression()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (feature_kernel): DotProductKernel(\n",
       "              (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
       "              (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (input_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "          (output_linear): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (LinNormAct_1): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "          (LinNormAct_2): Sequential(\n",
       "            (linear): Pass(\n",
       "              (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (norm): Sequential()\n",
       "            (activation): Pass(\n",
       "              (module): Expression()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_ema): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_mlp): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): GlobalPool()\n",
       "      (7): Sequential(\n",
       "        (0): Sequential()\n",
       "        (1): Expression()\n",
       "        (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (3): Sequential()\n",
       "        (4): Expression()\n",
       "        (5): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (6): Sequential()\n",
       "        (7): Expression()\n",
       "        (8): Linear(in_features=160, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = forge.config() # load configuration flags\n",
    "\n",
    "network = DynamicsEquivariantTransformer(\n",
    "        group=T(2),\n",
    "        dim_input=2,\n",
    "        dim_output=1,  # Potential term in Hamiltonian is scalar\n",
    "        dim_hidden=config.dim_hidden,\n",
    "        num_layers=config.num_layers,\n",
    "        num_heads=config.num_heads,\n",
    "        global_pool=True,\n",
    "        global_pool_mean=config.mean_pooling,\n",
    "        liftsamples=config.lift_samples,\n",
    "        kernel_dim=config.kernel_dim,\n",
    "        kernel_act=config.activation_function,\n",
    "        block_norm=config.block_norm,\n",
    "        output_norm=config.output_norm,\n",
    "        kernel_norm=config.kernel_norm,\n",
    "        kernel_type=config.kernel_type,\n",
    "        architecture=config.architecture,\n",
    "        attention_fn=config.attention_fn,\n",
    "    )\n",
    "\n",
    "model = DynamicsPredictor(network, debug=False, task=\"spring\", model_with_dict=config.model_with_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b883d36-4144-44c0-aa67-9c2c88e0606f",
   "metadata": {},
   "source": [
    "# Training and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb02b96-5439-4656-a39a-7437f60325b8",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b54ebe-8911-4c80-9315-2d3b43293338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure data\n",
    "data_config = SimpleNamespace(**{\n",
    "    'n_train': 3000,\n",
    "    'n_test': 2000,\n",
    "    'n_val': 2000,\n",
    "    'n_systems': 10000,\n",
    "    'data_path': './datasets/ODEDynamics/SpringDynamics/',\n",
    "    'sys_dim': 2,\n",
    "    'space_dim': 2,\n",
    "    'data_seed': 0,\n",
    "    'batch_size': 100,\n",
    "    'device': 0,\n",
    "    \"num_particles\": 6,\n",
    "    \"chunk_len\": 5,\n",
    "    \"load_preprocessed\":False,\n",
    "    \"nested_and_unshuffled\":False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19478184-e342-4743-ac02-6f861a102390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'spring_dynamics_data' from lie_transformer/configs/dynamics/spring_dynamics_data.py\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataloaders, data_name = fet.load(config.data_config, config = data_config)\n",
    "\n",
    "train_loader = dataloaders[\"train\"]\n",
    "test_loader = dataloaders[\"test\"]\n",
    "val_loader = dataloaders[\"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f4b1b-99d8-4fe7-9efa-f8e55292f453",
   "metadata": {},
   "source": [
    "### Set up training and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6845667-9747-4efa-b67b-70c898c80b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device, optimizer\n",
    "if torch.cuda.is_available():\n",
    "    device = f\"cuda:{config.device}\"\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "model_params = model.predictor.parameters()\n",
    "\n",
    "opt_learning_rate = config.learning_rate\n",
    "model_opt = torch.optim.Adam(\n",
    "    model_params, lr=opt_learning_rate, betas=(config.beta1, config.beta2)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            model_opt, config.train_epochs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246d0374-8e67-4f20-b3f4-64ef844aa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory management\n",
    "results_folder_name = \"demonstration\"\n",
    "checkpoint_dir = os.path.join(config.results_dir, results_folder_name.replace(\".\", \"_\"))\n",
    "\n",
    "experiment_folders = [f for f in os.listdir(checkpoint_dir)\n",
    "                          if not f.startswith('_') and not f.startswith('.')]\n",
    "\n",
    "if experiment_folders:\n",
    "    experiment_folder = int(sorted(experiment_folders, key=lambda x: int(x))[-1])\n",
    "    if not config.resume:\n",
    "        experiment_folder += 1\n",
    "else:\n",
    "    if config.resume:\n",
    "        raise ValueError(\"Can't resume since no experiments were run before in checkpoint\"\n",
    "                         \" dir '{}'.\".format(checkpoint_dir))\n",
    "    else:\n",
    "        experiment_folder = 1\n",
    "\n",
    "experiment_folder = os.path.join(checkpoint_dir, str(experiment_folder))\n",
    "if not config.resume:\n",
    "    os.mkdir(experiment_folder)\n",
    "\n",
    "logdir = experiment_folder\n",
    "\n",
    "checkpoint_name = os.path.join(logdir, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052f098-677d-43bd-997f-2159d4aaed7d",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7fea40-8a78-496f-849e-04e448eb4951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at epoch = 1, iter = 1\n",
      "saving model at epoch 0 before training ... \n",
      "Saving model training checkpoint to lie_transformer/checkpoints/demonstration/4/model.ckpt-0\n",
      "finished saving model at epoch 0 before training\n",
      "Number of model parameters: 841641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: time 4.423,  epoch: 1 [0 / 30]: mse:0.121947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 93\u001b[0m     reports \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     reports \u001b[38;5;241m=\u001b[39m parse_reports(reports)\n\u001b[1;32m     95\u001b[0m     reports[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_t\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m nested_to(data, device, torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reports \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         reports \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mreports\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mDynamicsPredictor.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m o \u001b[38;5;241m=\u001b[39m AttrDict()\n\u001b[1;32m     24\u001b[0m (z0, sys_params, ts), true_zs \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m---> 26\u001b[0m pred_zs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollout_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m mse \u001b[38;5;241m=\u001b[39m (pred_zs \u001b[38;5;241m-\u001b[39m true_zs)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mDynamicsPredictor._rollout_model\u001b[0;34m(self, z0, ts, sys_params, tol)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"inputs [z0: (bs, z_dim), ts: (bs, T), sys_params: (bs, n, c)]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03moutputs pred_zs: (bs, T, z_dim)\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m dynamics \u001b[38;5;241m=\u001b[39m Partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, sysP\u001b[38;5;241m=\u001b[39msys_params)\n\u001b[0;32m---> 18\u001b[0m zs \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrk4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py:79\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     76\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py:114\u001b[0m, in \u001b[0;36mFixedGridODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    112\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mcallback_step(t0, y0, dt)\n\u001b[0;32m--> 114\u001b[0m dy, f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m dy\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;129;01mand\u001b[39;00m t1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m t[j]:\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torchdiffeq/_impl/fixed_grid.py:29\u001b[0m, in \u001b[0;36mRK4._step_func\u001b[0;34m(self, func, t0, dt, t1, y0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, t0, dt, t1, y0):\n\u001b[1;32m     28\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m func(t0, y0, perturb\u001b[38;5;241m=\u001b[39mPerturb\u001b[38;5;241m.\u001b[39mNEXT \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperturb \u001b[38;5;28;01melse\u001b[39;00m Perturb\u001b[38;5;241m.\u001b[39mNONE)\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrk4_alt_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m, f0\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py:115\u001b[0m, in \u001b[0;36mrk4_alt_step_func\u001b[0;34m(func, t0, dt, t1, y0, f0, perturb)\u001b[0m\n\u001b[1;32m    113\u001b[0m k2 \u001b[38;5;241m=\u001b[39m func(t0 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m _one_third, y0 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m*\u001b[39m _one_third)\n\u001b[1;32m    114\u001b[0m k3 \u001b[38;5;241m=\u001b[39m func(t0 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m _two_thirds, y0 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m (k2 \u001b[38;5;241m-\u001b[39m k1 \u001b[38;5;241m*\u001b[39m _one_third))\n\u001b[0;32m--> 115\u001b[0m k4 \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPREV\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (k1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m (k2 \u001b[38;5;241m+\u001b[39m k3) \u001b[38;5;241m+\u001b[39m k4) \u001b[38;5;241m*\u001b[39m dt \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.125\u001b[39m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:197\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/lie_conv/dynamicsTrainer.py:22\u001b[0m, in \u001b[0;36mPartial.forward\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39mx):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mnfe \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m, in \u001b[0;36mDynamicsEquivariantTransformer.forward\u001b[0;34m(self, t, z, sysP, wgrad)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, z, sysP, wgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m     dynamics \u001b[38;5;241m=\u001b[39m HamiltonianDynamics(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m t, z: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_H(z, sysP), wgrad\u001b[38;5;241m=\u001b[39mwgrad\n\u001b[1;32m     84\u001b[0m     )\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/lie_conv/hamiltonian.py:20\u001b[0m, in \u001b[0;36mHamiltonianDynamics.forward\u001b[0;34m(self, t, z)\u001b[0m\n\u001b[1;32m     18\u001b[0m     D \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     19\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH(t,z)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;66;03m# elements in mb are independent, gives mb gradients\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     rg \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwgrad\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# riemannian gradient\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rg[:,D\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:],\u001b[38;5;241m-\u001b[39mrg[:,:D\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# symplectic gradient = SdH\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sg\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/lietransformer/lib/python3.9/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 1\n",
    "train_iter = (start_epoch - 1) * (\n",
    "        len(train_loader.dataset) // config.batch_size\n",
    "    ) + 1\n",
    "print(\"Starting training at epoch = {}, iter = {}\".format(start_epoch, train_iter))\n",
    "# Setup tensorboard writing\n",
    "summary_writer = SummaryWriter(logdir)\n",
    "\n",
    "train_reports = []\n",
    "report_all = {}\n",
    "report_all_val = {}\n",
    "\n",
    "# Saving model at epoch 0 before training\n",
    "print(\"saving model at epoch 0 before training ... \")\n",
    "save_checkpoint(checkpoint_name, 0, model, model_opt, loss=0.0)\n",
    "print(\"finished saving model at epoch 0 before training\")\n",
    "\n",
    "num_params = param_count(model)\n",
    "print(f\"Number of model parameters: {num_params}\")\n",
    "\n",
    "# Training\n",
    "start_t = time.time()\n",
    "\n",
    "total_train_iters = len(train_loader) * config.train_epochs\n",
    "iters_per_eval = max(1, int(total_train_iters / config.total_evaluations))\n",
    "\n",
    "assert (\n",
    "    config.n_train % min(config.batch_size, config.n_train) == 0\n",
    "), \"Batch size doesn't divide dataset size. Can be inaccurate for loss computation (see below).\"\n",
    "\n",
    "training_failed = False\n",
    "best_val_loss_so_far = 1e7\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, config.train_epochs + 1)):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = nested_to(\n",
    "            data, device, torch.float32\n",
    "        )  # the format is ((z0, sys_params, ts), true_zs) for data\n",
    "        true_zs = data[-1]\n",
    "        if config.model_with_dict:\n",
    "            outputs = model(data)\n",
    "        else:\n",
    "            pred_zs = model(data)\n",
    "            loss = (pred_zs - true_zs).pow(2).mean()\n",
    "            outputs = AttrDict({\"loss\": loss, \"prediction\": pred_zs})\n",
    "            outputs.reports = AttrDict({\"mse\": loss})\n",
    "\n",
    "        if torch.isnan(outputs.loss):\n",
    "            if not training_failed:\n",
    "                epoch_of_nan = epoch\n",
    "            if (epoch > epoch_of_nan + 1) and training_failed:\n",
    "                raise ValueError(\"Loss Nan-ed.\")\n",
    "            training_failed = True\n",
    "\n",
    "        model_opt.zero_grad()\n",
    "        outputs.loss.backward(retain_graph=False)\n",
    "\n",
    "        model_opt.step()\n",
    "\n",
    "        train_reports.append(parse_reports_cpu(outputs.reports))\n",
    "\n",
    "        if config.log_train_values:\n",
    "            reports = parse_reports(outputs.reports)\n",
    "            if batch_idx % config.report_loss_every == 0:\n",
    "                log_tensorboard(summary_writer, train_iter, reports, \"train/\")\n",
    "                print_reports(\n",
    "                    reports,\n",
    "                    start_t,\n",
    "                    epoch,\n",
    "                    batch_idx,\n",
    "                    len(train_loader.dataset) // config.batch_size,\n",
    "                    prefix=\"train\",\n",
    "                )\n",
    "                log_tensorboard(\n",
    "                    summary_writer,\n",
    "                    train_iter,\n",
    "                    {\"lr\": model_opt.param_groups[0][\"lr\"]},\n",
    "                    \"hyperparams/\",\n",
    "                )\n",
    "\n",
    "        # Do learning rate schedule steps per STEP for cosine_annealing_warmup\n",
    "        if config.lr_schedule == \"cosine_annealing_warmup\":\n",
    "            scheduler.step()\n",
    "\n",
    "        # Logging and evaluation\n",
    "        if (\n",
    "            train_iter % iters_per_eval == 0 or (train_iter == total_train_iters)\n",
    "        ) and config.log_val_test:  # batch_idx % config.evaluate_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                reports = evaluate(model, test_loader, device)\n",
    "                reports = parse_reports(reports)\n",
    "                reports[\"time\"] = time.time() - start_t\n",
    "                if report_all == {}:\n",
    "                    report_all = deepcopy(reports)\n",
    "\n",
    "                    for d in reports.keys():\n",
    "                        report_all[d] = [report_all[d]]\n",
    "                else:\n",
    "                    for d in reports.keys():\n",
    "                        report_all[d].append(reports[d])\n",
    "\n",
    "                log_tensorboard(summary_writer, train_iter, reports, \"test/\")\n",
    "                print_reports(\n",
    "                    reports,\n",
    "                    start_t,\n",
    "                    epoch,\n",
    "                    batch_idx,\n",
    "                    len(train_loader.dataset) // config.batch_size,\n",
    "                    prefix=\"test\",\n",
    "                )\n",
    "\n",
    "                # repeat for validation data\n",
    "                reports = evaluate(model, val_loader, device)\n",
    "                reports = parse_reports(reports)\n",
    "                reports[\"time\"] = time.time() - start_t\n",
    "                if report_all_val == {}:\n",
    "                    report_all_val = deepcopy(reports)\n",
    "\n",
    "                    for d in reports.keys():\n",
    "                        report_all_val[d] = [report_all_val[d]]\n",
    "                else:\n",
    "                    for d in reports.keys():\n",
    "                        report_all_val[d].append(reports[d])\n",
    "\n",
    "                log_tensorboard(summary_writer, train_iter, reports, \"val/\")\n",
    "                print_reports(\n",
    "                    reports,\n",
    "                    start_t,\n",
    "                    epoch,\n",
    "                    batch_idx,\n",
    "                    len(train_loader.dataset) // config.batch_size,\n",
    "                    prefix=\"val\",\n",
    "                )\n",
    "\n",
    "                if report_all_val[\"mse\"][-1] < best_val_loss_so_far:\n",
    "                    save_checkpoint(\n",
    "                        checkpoint_name,\n",
    "                        f\"early_stop\",\n",
    "                        model,\n",
    "                        model_opt,\n",
    "                        loss=outputs.loss,\n",
    "                    )\n",
    "                    best_val_loss_so_far = report_all_val[\"mse\"][-1]\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        train_iter += 1\n",
    "\n",
    "    # Do learning rate schedule steps per *epoch* for cosine_annealing\n",
    "    if config.lr_schedule == \"cosine_annealing\":\n",
    "        scheduler.step()\n",
    "\n",
    "    if epoch % config.save_check_points == 0:\n",
    "        save_checkpoint(\n",
    "            checkpoint_name, train_iter, model, model_opt, loss=outputs.loss\n",
    "        )\n",
    "\n",
    "    dd.io.save(logdir + \"/results_dict_train.h5\", train_reports)\n",
    "    dd.io.save(logdir + \"/results_dict.h5\", report_all)\n",
    "    dd.io.save(logdir + \"/results_dict_val.h5\", report_all_val)\n",
    "\n",
    "# always save final model\n",
    "save_checkpoint(checkpoint_name, train_iter, model, model_opt, loss=outputs.loss)\n",
    "\n",
    "if config.save_test_predictions:\n",
    "    print(\"Starting to make model predictions on test sets for *final model*.\")\n",
    "    for chunk_len in [5, 100]:\n",
    "        start_t_preds = time.time()\n",
    "        data_config = SimpleNamespace(\n",
    "            **{\n",
    "                **config.__dict__[\"__flags\"],\n",
    "                **{\"chunk_len\": chunk_len, \"batch_size\": 500},\n",
    "            }\n",
    "        )\n",
    "        dataloaders, data_name = fet.load(config.data_config, config=data_config)\n",
    "        test_loader_preds = dataloaders[\"test\"]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            true = []\n",
    "            num_datapoints = 0\n",
    "            for idx, d in enumerate(test_loader_preds):\n",
    "                true.append(d[-1])\n",
    "                d = nested_to(d, device, torch.float32)\n",
    "                outputs = model(d)\n",
    "\n",
    "                pred_zs = outputs.prediction\n",
    "                preds.append(pred_zs)\n",
    "\n",
    "                num_datapoints += len(pred_zs)\n",
    "\n",
    "                if num_datapoints >= 2000:\n",
    "                    break\n",
    "\n",
    "            preds = torch.cat(preds, dim=0).cpu()\n",
    "            true = torch.cat(true, dim=0).cpu()\n",
    "\n",
    "            save_dir = osp.join(logdir, f\"traj_preds_{chunk_len}_steps_2k_test.pt\")\n",
    "            torch.save(preds, save_dir)\n",
    "\n",
    "            save_dir = osp.join(logdir, f\"traj_true_{chunk_len}_steps_2k_test.pt\")\n",
    "            torch.save(true, save_dir)\n",
    "\n",
    "            print(\n",
    "                f\"Completed making test predictions for chunk_len = {chunk_len} in {time.time() - start_t_preds:.2f} seconds.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b915f53-7d15-49d5-8f4d-b63fb5103c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
