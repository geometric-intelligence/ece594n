To run the Poincaré word embeddings, save the html files and open them in a browser.

The hyperplane image is obtained by running the code in the paper's github repo. The paper outlines a way to use hyperplanes to implement the softmax function (MLR) on the Poincaré (hyperbolic) model.

The hyp_vs_euc_binary_MLR image compares the binary classification obtained from the hyperbolic MLR (left) and the Euclidean MLR (right). We see that the hyperbolic MLR is able to learn a curved hyperplane to classify a small section of data points. This allows the hyperbolic model to more accurately classify tree-like embedded data. In this example, a database of nouns is embedded on the Poincaré model using one of the provided html files. Each word is assigned to a point on the Poincaré model, where points closer to the edge of the model correspond to words with more specific classifications. This embedding accurately depicts the tree-like structure of the nouns in the database. For example, the embedding of the word 'snake' would have a higher radius than 'reptile,' but it would have a similar angle from the center of the circle. We use hyperbolic embeddings for tree-like data because distances in hyperbolic space increase exponentially with radius. In other words, more specific branches are effectively embedded exponentially farther away from other branches. Following this hyperbolic model, the hyperbolic MLR is able to accurately learn the classification for a specific branch because its softmax function utilizes a (seemingly curved) hyperplane in hyperbolic space. The paper shows that the hyperbolic MLR outperforms the Euclidean one.