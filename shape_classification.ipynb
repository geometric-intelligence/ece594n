{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Field Networks\n",
    "\n",
    "Implementation of shape classification demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from math import pi, sqrt\n",
    "import tensorfieldnetworks.layers as layers\n",
    "import tensorfieldnetworks.utils as utils\n",
    "from tensorfieldnetworks.utils import FLOAT_TYPE\n",
    "\n",
    "tetris = [[(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 1, 0)],  # chiral_shape_1\n",
    "          [(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, -1, 0)], # chiral_shape_2\n",
    "          [(0, 0, 0), (1, 0, 0), (0, 1, 0), (1, 1, 0)],  # square\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3)],  # line\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)],  # corner\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 0)],  # T\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 1)],  # zigzag\n",
    "          [(0, 0, 0), (1, 0, 0), (1, 1, 0), (2, 1, 0)]]  # L\n",
    "\n",
    "dataset = [np.array(points_) for points_ in tetris]\n",
    "num_classes = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radial basis functions\n",
    "rbf_low = 0.0\n",
    "rbf_high = 3.5\n",
    "rbf_count = 4\n",
    "rbf_spacing = (rbf_high - rbf_low) / rbf_count\n",
    "centers = tf.cast(tf.lin_space(rbf_low, rbf_high, rbf_count), FLOAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r : [N, 3]\n",
    "r = tf.placeholder(FLOAT_TYPE, shape=(4, 3))\n",
    "\n",
    "# rij : [N, N, 3]\n",
    "rij = utils.difference_matrix(r)\n",
    "\n",
    "# dij : [N, N]\n",
    "dij = utils.distance_matrix(r)\n",
    "\n",
    "# rbf : [N, N, rbf_count]\n",
    "gamma = 1. / rbf_spacing\n",
    "rbf = tf.exp(-gamma * tf.square(tf.expand_dims(dij, axis=-1) - centers))\n",
    "\n",
    "layer_dims = [1, 4, 4, 4]\n",
    "num_layers = len(layer_dims) - 1\n",
    "\n",
    "# embed : [N, layer1_dim, 1]\n",
    "with tf.variable_scope(None, \"embed\"):\n",
    "    embed = layers.self_interaction_layer_without_biases(tf.ones(shape=(4, 1, 1)), layer_dims[0])\n",
    "\n",
    "input_tensor_list = {0: [embed]}\n",
    "\n",
    "for layer, layer_dim in enumerate(layer_dims[1:]):\n",
    "    with tf.variable_scope(None, 'layer' + str(layer), values=[input_tensor_list]):\n",
    "        input_tensor_list = layers.convolution(input_tensor_list, rbf, rij)\n",
    "        input_tensor_list = layers.concatenation(input_tensor_list)\n",
    "        input_tensor_list = layers.self_interaction(input_tensor_list, layer_dim)\n",
    "        input_tensor_list = layers.nonlinearity(input_tensor_list)\n",
    "\n",
    "tfn_scalars = input_tensor_list[0][0]\n",
    "tfn_output_shape = tfn_scalars.get_shape().as_list()\n",
    "tfn_output = tf.reduce_mean(tf.squeeze(tfn_scalars), axis=0)\n",
    "fully_connected_layer = tf.get_variable('fully_connected_weights', \n",
    "                                        [tfn_output_shape[-2], len(dataset)], dtype=FLOAT_TYPE)\n",
    "output_biases = tf.get_variable('output_biases', [len(dataset)], dtype=FLOAT_TYPE)\n",
    "\n",
    "# output : [num_classes]\n",
    "output = tf.einsum('xy,x->y', fully_connected_layer, tfn_output) + output_biases\n",
    "\n",
    "tf_label = tf.placeholder(tf.int32)\n",
    "\n",
    "# truth : [num_classes]\n",
    "truth = tf.one_hot(tf_label, num_classes)\n",
    "\n",
    "# loss : []\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=truth, logits=output)\n",
    "\n",
    "optim = tf.train.AdamOptimizer(learning_rate=1.e-3)\n",
    "\n",
    "train_op = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: validation loss = 2.189\n",
      "Epoch 100: validation loss = 1.324\n",
      "Epoch 200: validation loss = 0.407\n",
      "Epoch 300: validation loss = 0.037\n",
      "Epoch 400: validation loss = 0.015\n",
      "Epoch 500: validation loss = 0.008\n",
      "Epoch 600: validation loss = 0.005\n",
      "Epoch 700: validation loss = 0.003\n",
      "Epoch 800: validation loss = 0.002\n",
      "Epoch 900: validation loss = 0.001\n",
      "Epoch 1000: validation loss = 0.001\n",
      "Epoch 1100: validation loss = 0.001\n",
      "Epoch 1200: validation loss = 0.000\n",
      "Epoch 1300: validation loss = 0.000\n",
      "Epoch 1400: validation loss = 0.000\n",
      "Epoch 1500: validation loss = 0.000\n",
      "Epoch 1600: validation loss = 0.000\n",
      "Epoch 1700: validation loss = 0.000\n",
      "Epoch 1800: validation loss = 0.000\n",
      "Epoch 1900: validation loss = 0.000\n",
      "Epoch 2000: validation loss = 0.000\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 2001\n",
    "print_freq = 100\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# training\n",
    "for epoch in xrange(max_epochs):    \n",
    "    loss_sum = 0.\n",
    "    for label, shape in enumerate(dataset):\n",
    "        loss_value, _ = sess.run([loss, train_op], feed_dict={r: shape, tf_label: label})\n",
    "        loss_sum += loss_value\n",
    "        \n",
    "    if epoch % print_freq == 0:\n",
    "        print(\"Epoch %d: validation loss = %.3f\" % (epoch, loss_sum / len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState()\n",
    "test_set_size = 25\n",
    "predictions = [list() for i in range(len(dataset))]\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for i in range(test_set_size):\n",
    "    for label, shape in enumerate(dataset):\n",
    "        rotation = utils.random_rotation_matrix(rng)\n",
    "        rotated_shape = np.dot(shape, rotation)\n",
    "        translation = np.expand_dims(np.random.uniform(low=-3., high=3., size=(3)), axis=0)\n",
    "        translated_shape = rotated_shape + translation\n",
    "        output_label = sess.run(tf.argmax(output), \n",
    "                                feed_dict={r: rotated_shape, tf_label: label})\n",
    "        total_predictions += 1\n",
    "        if output_label == label:\n",
    "            correct_predictions += 1\n",
    "print('Test accuracy: %f' % (float(correct_predictions) / total_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
