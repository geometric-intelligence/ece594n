{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Xinling Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we study tensor field neural networks [1], which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. Convolutional neural networks are translation-equivariant, which means that features can be indetified anywhere in an input image. However, in order to recognize features with arbitrary 3D rotations and their orientations, data augmentation is required in traditional convolutional neural networks. To eliminate the need for data augmentation, the authors use continuous equivalent filters constructed from spherical harmonics, which enjoy richer equivariance: the symmetries of 3D Euclidean space.\n",
    "\n",
    "This work builds upon Harmonic Networks [2], which achieves 2D rotation equivariance using discrete convolutions and filters composed of circular harmonics and SchNet [3], which presents a rotation-invariant network using continuous convolutions. Similar problems of invariance or equivalence under specific input transformations have been dealt with in previous work. G-CNNs [4] guarantee equivariance with respect to finite symmetry groups (unlike the continuous groups in this work). Spherical CNNs [5] use spherical harmonics and Wigner D-matrices but only address spherical signals (2D data on the surface of a sphere). Differ from these works, tensor field neural networks can achieve 3D rotation- and translation- equivarianence.\n",
    "\n",
    "The rest of the notebook is organized as follows. In Section 2, we present a brief overview of group representations and equivariance in 3D, then we introduce tensor field neural network layers. Next, we implement the tensor field neural network model using Geomstats [6] and Tensorflow, see Section 3 and layer.py. Furthermore, we demonstrate the 3D rotation- and translation-equivariance of the model using a toy 3D Teris shapes dataset, we also present the results using a novel preshape space method implemented in the Geomstats package [6] in another notebook Preshape_space.ipynb. Finally we compare the results from the tensor field networks with the results from preshape space method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><img src='images/motivation.png', width=800, height=800>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><img src='images/motivation.png', width=800, height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Background and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Group Representations and Equivariance in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A representation $D$ of a group $G$ is a function from $G$ to square matrices such that for all $g, h \\in G$,\n",
    "$$\n",
    "D(g) D(h)=D(g h)\n",
    "$$\n",
    "A function $\\mathcal{L}: \\mathcal{X} \\rightarrow \\mathcal{Y}$ (for vector spaces $\\mathcal{X}$ and $\\mathcal{Y}$ ) is equivariant with respect to a group $G$ and group representations $D^{\\mathcal{X}}$ and $D^{\\mathcal{Y}}$ if for all $g \\in G$,\n",
    "$$\n",
    "\\mathcal{L} \\circ D^{\\mathcal{X}}(g)=D^{\\mathcal{Y}}(g) \\circ \\mathcal{L}\n",
    "$$\n",
    "Tensor field networks act on points with associated features. A layer $\\mathcal{L}$ takes a finite set $S$ of vectors in $\\mathbb{R}^{3}$ and a vector in $\\mathcal{X}$ at each point in $S$ and outputs a vector in $\\mathcal{Y}$ at each point in $S$, where $\\mathcal{X}$ and $\\mathcal{Y}$ are vector spaces. We write this as\n",
    "$$\n",
    "\\mathcal{L}\\left(\\vec{r}_{a}, x_{a}\\right)=\\left(\\vec{r}_{a}, y_{a}\\right)\n",
    "$$\n",
    "where $\\vec{r}_{a} \\in \\mathbb{R}^{3}$ are the point coordinates and $x_{a} \\in \\mathcal{X}, y_{a} \\in \\mathcal{Y}$ are the feature vectors ( $a$ being an indexing scheme over the points in $S$ ). This combination of $\\mathbb{R}^{3}$ with another vector space can be written as $\\mathbb{R}^{3} \\oplus \\mathcal{X}$, where $\\oplus$ refers to concatenation.\n",
    "Next we study the equivariance in 3D.\n",
    "1. Permutation equivariance: $\\mathcal{L} \\circ \\mathcal{P}_{\\sigma}=\\mathcal{P}_{\\sigma} \\circ \\mathcal{L}$, where $\\mathcal{P}_{\\sigma}\\left(\\vec{r}_{a}, x_{a}\\right):=\\left(\\vec{r}_{\\sigma(a)}, x_{\\sigma(a)}\\right)$ and $\\sigma$ permutes the points to which the indices refer.\n",
    "2. Translation equivariance: $\\mathcal{L} \\circ \\mathcal{T}_{\\vec{t}}=\\mathcal{T}_{\\vec{t}} \\circ \\mathcal{L}$, where $\\mathcal{T}_{\\vec{t}}\\left(\\vec{r}_{a}, x_{a}\\right):=\\left(\\vec{r}_{a}+\\vec{t}, x_{a}\\right)$.\n",
    "3. Rotation equivariance: Let $D^{\\mathcal{X}}$ be a representation of $S O(3)$ on a vector space $\\mathcal{X}\\left(\\right.$ and $D^{\\mathcal{Y}}$ on $\\mathcal{Y})$. Acting with $g \\in S O(3)$ on $\\vec{r} \\in \\mathbb{R}^{3}$ we write as $\\mathcal{R}(g) \\vec{r}$, and acting on $x \\in \\mathcal{X}$ gives $D^{\\mathcal{X}}(g) x$. Then the condition for rotation equivariance is $\\mathcal{L} \\circ\\left[\\mathcal{R}(g) \\oplus D^{\\mathcal{X}}(g)\\right]=\\left[\\mathcal{R}(g) \\oplus D^{\\mathcal{Y}}(g)\\right] \\circ \\mathcal{L}$, where $\\left[\\mathcal{R}(g) \\oplus D^{\\mathcal{X}}(g)\\right]\\left(\\vec{r}_{a}, x_{a}\\right)=\\left(\\mathcal{R}(g) \\vec{r}_{a}, D^{\\mathcal{X}}(g) x_{a}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tensor Field Neural Network Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output $V_{a c m}^{(l)}$ of each layer of a tensor field network is a finite set $S$ of points in $\\mathbb{R}^{3}$ and a vector in a representation of $S O(3)$ associated with each point. This object $V_{a c m}^{(l)}$ is implemented as a dictionary with key $l$ (the rotation order) of multidimensional arrays each with shapes $\\left[|S|, n_{l}, 2 l+1\\right]$ (where $n_{l}$ is the number of channels) corresponding to [point index, channel index, representation index]. The following figure shows two point masses with velocity (purple) and acceleration (orange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><img src='images/tensor.png', width=800, height=800>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><img src='images/tensor.png', width=800, height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define three tensor field network layers. We refer to [1] to check more details about why they are equivariant.\n",
    "1. Point convolution: A given input inhabits one representation, a filter inhabits another, and together these produce outputs at possibly many rotation orders. We can put everything together into our pointwise convolution layer definition:\n",
    "$$\n",
    "\\mathcal{L}_{a c m_{o}}^{\\left(l_{o}\\right)}\\left(\\vec{r}_{a}, V_{a c m_{i}}^{\\left(l_{i}\\right)}\\right):=\\sum_{m_{f}, m_{i}} C_{\\left(l_{f}, m_{f}\\right)\\left(l_{i}, m_{i}\\right)}^{\\left(l_{o}, m_{o}\\right)} \\sum_{b \\in S} F_{c m_{f}}^{\\left(l_{f}, l_{i}\\right)}\\left(\\vec{r}_{a b}\\right) V_{b c m_{i}}^{\\left(l_{i}\\right)}\n",
    "$$\n",
    "where $\\vec{r}_{a b}:=\\vec{r}_{a}-\\vec{r}_{b}$ and the subscripts $i, f$, and $o$ denote the representations of the input, filter, and output, respectively.\n",
    "2. Self-interaction: Self-interaction layers are analogous to $1 \\times 1$ convolutions, and they act like $l=0$ (scalar) filters:\n",
    "$$\n",
    "\\sum_{c^{\\prime}} W_{c c^{\\prime}}^{(l)} V_{a c^{\\prime} m}^{(l)}\n",
    "$$\n",
    "3. Nonlinearity: Nonlinearity layer acts as a scalar transform in the $l$ spaces (that is, along the $m$ dimension). For $l=0$ channels, we can use\n",
    "$\\eta^{(0)}\\left(V_{a c}^{(0)}+b_{c}^{(0)}\\right) \\quad$ and $\\quad \\eta^{(l)}\\left(\\|V\\|_{a c}^{(l)}+b_{c}^{(l)}\\right) V_{a c m}^{(l)} \\quad$ where $\\quad\\|V\\|_{a c}^{(l)}:=\\sqrt{\\sum_{m}\\left|V_{a c m}^{(l)}\\right|^{2}}$\n",
    "for some functions $\\eta^{(l)}: \\mathbb{R} \\rightarrow \\mathbb{R}$ (which can be different for each $l$ ) and biases $b_{c}^{(l)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><img src='images/network.png', width=800, height=800>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><img src='images/network.png', width=800, height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using tensorflow backend\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import os\n",
    "os.environ['GEOMSTATS_BACKEND'] = 'tensorflow'\n",
    "import geomstats.backend as gs # import the Geomstats tensorflow 2.2 backend\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "FLOAT_TYPE = gs.float32\n",
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "def get_eijk():\n",
    "    \"\"\"\n",
    "    Constant Levi-Civita tensor\n",
    "    Returns:\n",
    "        tf.Tensor of shape [3, 3, 3]\n",
    "    \"\"\"\n",
    "    eijk_ = np.zeros((3, 3, 3))\n",
    "    eijk_[0, 1, 2] = eijk_[1, 2, 0] = eijk_[2, 0, 1] = 1.\n",
    "    eijk_[0, 2, 1] = eijk_[2, 1, 0] = eijk_[1, 0, 2] = -1.\n",
    "    return tf.constant(eijk_, dtype=FLOAT_TYPE)\n",
    "\n",
    "\n",
    "def norm_with_epsilon(input_tensor, axis=None, keep_dims=False):\n",
    "    \"\"\"\n",
    "    Regularized norm\n",
    "    Args:\n",
    "        input_tensor: tf.Tensor\n",
    "    Returns:\n",
    "        tf.Tensor normed over axis\n",
    "    \"\"\"\n",
    "    return gs.sqrt(gs.maximum(tf.reduce_sum(tf.square(input_tensor), axis=axis, keep_dims=keep_dims), EPSILON))\n",
    "\n",
    "\n",
    "def ssp(x):\n",
    "    \"\"\"\n",
    "    Shifted soft plus nonlinearity.\n",
    "    Args:\n",
    "        x: tf.Tensor\n",
    "    Returns:\n",
    "        tf.Tensor of same shape as x \n",
    "   \"\"\"\n",
    "    return gs.log(0.5 * gs.exp(x) + 0.5)\n",
    "\n",
    "\n",
    "def rotation_equivariant_nonlinearity(x, nonlin=ssp, biases_initializer=None):\n",
    "    \"\"\"\n",
    "    Rotation equivariant nonlinearity.\n",
    "    The -1 axis is assumed to be M index (of which there are 2 L + 1 for given L).\n",
    "    Args:\n",
    "        x: tf.Tensor with channels as -2 axis and M as -1 axis.\n",
    "    Returns:\n",
    "        tf.Tensor of same shape as x with 3d rotation-equivariant nonlinearity applied.\n",
    "    \"\"\"\n",
    "    if biases_initializer is None:\n",
    "        biases_initializer = tf.constant_initializer(0.)\n",
    "    shape = x.get_shape().as_list()\n",
    "    channels = shape[-2]\n",
    "    representation_index = shape[-1]\n",
    "\n",
    "    biases = tf.get_variable('biases',\n",
    "                             [channels],\n",
    "                             dtype=FLOAT_TYPE,\n",
    "                             initializer=biases_initializer)\n",
    "\n",
    "    if representation_index == 1:\n",
    "        return nonlin(x)\n",
    "    else:\n",
    "        norm = norm_with_epsilon(x, axis=-1)\n",
    "        nonlin_out = nonlin(gs.nn.bias_add(norm, biases))\n",
    "        factor = gs.divide(nonlin_out, norm)\n",
    "        # Expand dims for representation index.\n",
    "        return tf.multiply(x, gs.expand_dims(factor, axis=-1))\n",
    "    \n",
    "\n",
    "\n",
    "def difference_matrix(geometry):\n",
    "    \"\"\"\n",
    "    Get relative vector matrix for array of shape [N, 3].\n",
    "    Args:\n",
    "        geometry: tf.Tensor with Cartesian coordinates and shape [N, 3]\n",
    "    Returns:\n",
    "        Relative vector matrix with shape [N, N, 3]\n",
    "    \"\"\"\n",
    "    # [N, 1, 3]\n",
    "    ri = gs.expand_dims(geometry, axis=1)\n",
    "    # [1, N, 3]\n",
    "    rj = gs.expand_dims(geometry, axis=0)\n",
    "    # [N, N, 3]\n",
    "    rij = ri - rj\n",
    "    return rij\n",
    "\n",
    "\n",
    "def distance_matrix(geometry):\n",
    "    \"\"\"\n",
    "    Get relative distance matrix for array of shape [N, 3].\n",
    "    Args:\n",
    "        geometry: tf.Tensor with Cartesian coordinates and shape [N, 3]\n",
    "    Returns:\n",
    "        Relative distance matrix with shape [N, N]\n",
    "    \"\"\"\n",
    "    # [N, N, 3]\n",
    "    rij = difference_matrix(geometry)\n",
    "    # [N, N]\n",
    "    dij = norm_with_epsilon(rij, axis=-1)\n",
    "    return dij\n",
    "\n",
    "\n",
    "def random_rotation_matrix(numpy_random_state):\n",
    "    \"\"\"\n",
    "    Generates a random 3D rotation matrix from axis and angle.\n",
    "    Args:\n",
    "        numpy_random_state: numpy random state object\n",
    "    Returns:\n",
    "        Random rotation matrix.\n",
    "    \"\"\"\n",
    "    rng = numpy_random_state\n",
    "    axis = rng.randn(3)\n",
    "    axis /= np.linalg.norm(axis) + EPSILON\n",
    "    theta = 2 * np.pi * rng.uniform(0.0, 1.0)\n",
    "    return rotation_matrix(axis, theta)\n",
    "\n",
    "\n",
    "def rotation_matrix(axis, theta):\n",
    "    return scipy.linalg.expm(np.cross(np.eye(3), axis * theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 3D Tetris Shape Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset only contains 8 shapes, we evaluate the predition accuracy of tensor field networks on testing data set that is created by rotating and translating the training dataset. We use a 3-module network that includes the following for every module: all possible paths with l = 0 and l = 1 convolutions, concatenation, a self-interaction layer, and a rotation-equivariant nonlinearity. We only use the l = 0 output of the network since the shape classes are invariant under rotation and hence scalars. To get a classification from the l = 0 output of the network, we sum over the feature vectors of all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><img src='images/data.png', width=800, height=800>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><img src='images/data.png', width=800, height=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tensor Field Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import random\n",
    "from math import pi, sqrt\n",
    "import layers\n",
    "\n",
    "tetris = [[(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 1, 0)],  # chiral_shape_1\n",
    "          [(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, -1, 0)], # chiral_shape_2\n",
    "          [(0, 0, 0), (1, 0, 0), (0, 1, 0), (1, 1, 0)],  # square\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3)],  # line\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0)],  # corner\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 0)],  # T\n",
    "          [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 1)],  # zigzag\n",
    "          [(0, 0, 0), (1, 0, 0), (1, 1, 0), (2, 1, 0)]]  # L\n",
    "\n",
    "dataset = [np.array(points_) for points_ in tetris]\n",
    "num_classes = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-abdf04168f93>:33: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From <ipython-input-2-abdf04168f93>:33: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/xlyu/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From /Users/xlyu/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-8a9b3076eb42>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: From <ipython-input-5-8a9b3076eb42>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# radial basis functions\n",
    "rbf_low = 0.0\n",
    "rbf_high = 3.5\n",
    "rbf_count = 4\n",
    "rbf_spacing = (rbf_high - rbf_low) / rbf_count\n",
    "centers = tf.cast(tf.lin_space(rbf_low, rbf_high, rbf_count), FLOAT_TYPE)\n",
    "\n",
    "# r : [N, 3]\n",
    "r = tf.placeholder(FLOAT_TYPE, shape=(4, 3))\n",
    "\n",
    "# rij : [N, N, 3]\n",
    "rij = difference_matrix(r)\n",
    "\n",
    "# dij : [N, N]\n",
    "dij = distance_matrix(r)\n",
    "\n",
    "# rbf : [N, N, rbf_count]\n",
    "gamma = 1. / rbf_spacing\n",
    "rbf = tf.exp(-gamma * tf.square(tf.expand_dims(dij, axis=-1) - centers))\n",
    "\n",
    "layer_dims = [1, 4, 4, 4]\n",
    "num_layers = len(layer_dims) - 1\n",
    "\n",
    "# embed : [N, layer1_dim, 1]\n",
    "with tf.variable_scope(None, \"embed\"):\n",
    "    embed = layers.self_interaction_layer_without_biases(tf.ones(shape=(4, 1, 1)), layer_dims[0])\n",
    "\n",
    "input_tensor_list = {0: [embed]}\n",
    "\n",
    "for layer, layer_dim in enumerate(layer_dims[1:]):\n",
    "    with tf.variable_scope(None, 'layer' + str(layer), values=[input_tensor_list]):\n",
    "        input_tensor_list = layers.convolution(input_tensor_list, rbf, rij)\n",
    "        input_tensor_list = layers.concatenation(input_tensor_list)\n",
    "        input_tensor_list = layers.self_interaction(input_tensor_list, layer_dim)\n",
    "        input_tensor_list = layers.nonlinearity(input_tensor_list)\n",
    "\n",
    "tfn_scalars = input_tensor_list[0][0]\n",
    "tfn_output_shape = tfn_scalars.get_shape().as_list()\n",
    "tfn_output = tf.reduce_mean(tf.squeeze(tfn_scalars), axis=0)\n",
    "fully_connected_layer = tf.get_variable('fully_connected_weights', \n",
    "                                        [tfn_output_shape[-2], len(dataset)], dtype=FLOAT_TYPE)\n",
    "output_biases = tf.get_variable('output_biases', [len(dataset)], dtype=FLOAT_TYPE)\n",
    "\n",
    "# output : [num_classes]\n",
    "output = tf.einsum('xy,x->y', fully_connected_layer, tfn_output) + output_biases\n",
    "\n",
    "tf_label = tf.placeholder(tf.int32)\n",
    "\n",
    "# truth : [num_classes]\n",
    "truth = tf.one_hot(tf_label, num_classes)\n",
    "\n",
    "# loss : []\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=truth, logits=output)\n",
    "\n",
    "optim = tf.train.AdamOptimizer(learning_rate=1.e-3)\n",
    "\n",
    "train_op = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: validation loss = 2.170\n",
      "Epoch 100: validation loss = 1.254\n",
      "Epoch 200: validation loss = 0.639\n",
      "Epoch 300: validation loss = 0.352\n",
      "Epoch 400: validation loss = 0.070\n",
      "Epoch 500: validation loss = 0.021\n",
      "Epoch 600: validation loss = 0.009\n",
      "Epoch 700: validation loss = 0.005\n",
      "Epoch 800: validation loss = 0.003\n",
      "Epoch 900: validation loss = 0.002\n",
      "Epoch 1000: validation loss = 0.001\n",
      "Epoch 1100: validation loss = 0.001\n",
      "Epoch 1200: validation loss = 0.000\n",
      "Epoch 1300: validation loss = 0.000\n",
      "Epoch 1400: validation loss = 0.000\n",
      "Epoch 1500: validation loss = 0.000\n",
      "Epoch 1600: validation loss = 0.000\n",
      "Epoch 1700: validation loss = 0.000\n",
      "Epoch 1800: validation loss = 0.000\n",
      "Epoch 1900: validation loss = 0.000\n",
      "Epoch 2000: validation loss = 0.000\n",
      "CPU times: user 57.9 s, sys: 19.2 s, total: 1min 17s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_epochs = 2001\n",
    "print_freq = 100\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# training\n",
    "for epoch in range(max_epochs):    \n",
    "    loss_sum = 0.\n",
    "    for label, shape in enumerate(dataset):\n",
    "        loss_value, _ = sess.run([loss, train_op], feed_dict={r: shape, tf_label: label})\n",
    "        loss_sum += loss_value\n",
    "        \n",
    "    if epoch % print_freq == 0:\n",
    "        print(\"Epoch %d: validation loss = %.3f\" % (epoch, loss_sum / len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.000000\n",
      "CPU times: user 1min 35s, sys: 8.85 s, total: 1min 44s\n",
      "Wall time: 34.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rng = np.random.RandomState()\n",
    "test_set_size = 25 # we have 25 x 8 test shapes\n",
    "predictions = [list() for i in range(len(dataset))]\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for i in range(test_set_size):\n",
    "    for label, shape in enumerate(dataset):\n",
    "        rotation = random_rotation_matrix(rng)\n",
    "        rotated_shape = np.dot(shape, rotation)\n",
    "        translation = np.expand_dims(np.random.uniform(low=-3., high=3., size=(3)), axis=0)\n",
    "        translated_shape = rotated_shape + translation\n",
    "        output_label = sess.run(gs.argmax(output), \n",
    "                                feed_dict={r: translated_shape, tf_label: label})\n",
    "        total_predictions += 1\n",
    "        if output_label == label:\n",
    "            correct_predictions += 1\n",
    "print('Test accuracy: %f' % (float(correct_predictions) / total_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Geomstats: Preshape Space Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point clouds can also be treated as landmarks, then we can project point cloud to the preshape space and check which shapes it align the best. In the tetris shape classification task, the workflow can be described as follows:\n",
    "1. Compute the tetris shapes projections on the preshape space.\n",
    "2. Given an input rotated and translated tetris shape, compute its projection on the preshape space. Then we align the projection to the projected tetris shapes.\n",
    "3. Find the best alignment.\n",
    "\n",
    "Check more details in the notebook Preshape_space.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results from two notebooks, we can see that both tensor field networks and preshape space method achieve $100\\%$ accuracy in the 3D tetris shape classification task, which is sufficient to demonstrate that they are 3D rotation- and translation- equivalent. And preshape space method is much faster than the tensor field networks (1.85s vs more than 2mins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have explained the theory of tensor field networks and demonstrated their capabilities of 3D rotation- and translation- equivarianence using a simple classification example. We also propose a simple yet very efficient preshape space method that can be easily implemented by Geomstats. The results show that the preshape space method is much faster than the tensor field networks. However, the 3D tetris shape classification is not a hard task since the input point clouds are just $4 \\times 3$ matrices, in the future, it'll be very interesting to see if preshape space method and Geomstats can handle more complex point clouds data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Thomas, Nathaniel, et al. \"Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.\" arXiv preprint arXiv:1802.08219 (2018).\n",
    "\n",
    "[2] Worrall, Daniel E., et al. \"Harmonic networks: Deep translation and rotation equivariance.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[3] Sch√ºtt, Kristof T., et al. \"Quantum-chemical insights from deep tensor neural networks.\" Nature communications 8.1 (2017): 1-8.\n",
    "\n",
    "[4] Cohen, Taco, and Max Welling. \"Group equivariant convolutional networks.\" International conference on machine learning. PMLR, 2016.\n",
    "\n",
    "[5] Cohen, Taco S., et al. \"Spherical cnns.\" arXiv preprint arXiv:1801.10130 (2018).\n",
    "\n",
    "[6] Miolane, Nina, et al. \"Geomstats: a Python package for Riemannian geometry in machine learning.\" Journal of Machine Learning Research 21.223 (2020): 1-9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
